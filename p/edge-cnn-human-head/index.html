<!doctype html><html lang=zh-tw><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="2021 Spring Edge AI Midterm Report"><title>A CNN-Based Human Head Detection Algorithm Implemented on Edge AI Chip</title><link rel=canonical href=https://blog.jeffery.tk/p/edge-cnn-human-head/><link rel=stylesheet href=/scss/style.min.css><meta property="og:title" content="A CNN-Based Human Head Detection Algorithm Implemented on Edge AI Chip"><meta property="og:description" content="2021 Spring Edge AI Midterm Report"><meta property="og:url" content="https://blog.jeffery.tk/p/edge-cnn-human-head/"><meta property="og:site_name" content="Jeffery's Blog"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="cnn"><meta property="article:tag" content="ntust"><meta property="article:tag" content="human head detection"><meta property="article:tag" content="edge ai chip"><meta property="article:tag" content="report"><meta property="article:published_time" content="2021-04-20T00:00:00+00:00"><meta property="article:modified_time" content="2021-04-20T00:00:00+00:00"><meta name=twitter:title content="A CNN-Based Human Head Detection Algorithm Implemented on Edge AI Chip"><meta name=twitter:description content="2021 Spring Edge AI Midterm Report"><link rel="shortcut icon" href=/favicon.png></head><body class="article-page has-toc"><script>(function(){const colorSchemeKey='StackColorScheme';if(!localStorage.getItem(colorSchemeKey)){localStorage.setItem(colorSchemeKey,"auto");}})();</script><script>(function(){const colorSchemeKey='StackColorScheme';const colorSchemeItem=localStorage.getItem(colorSchemeKey);const supportDarkMode=window.matchMedia('(prefers-color-scheme: dark)').matches===true;if(colorSchemeItem=='dark'||colorSchemeItem==='auto'&&supportDarkMode){document.documentElement.dataset.scheme='dark';}else{document.documentElement.dataset.scheme='light';}})();</script><div class="container main-container flex
extended"><div id=article-toolbar><a href=https://blog.jeffery.tk/ class=back-home><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="15 6 9 12 15 18"/></svg><span>Back</span></a></div><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/edge-ai/>Edge AI</a>
<a href=/categories/paper/>Paper</a></header><h2 class=article-title><a href=/p/edge-cnn-human-head/>A CNN-Based Human Head Detection Algorithm Implemented on Edge AI Chip</a></h2><h3 class=article-subtitle>2021 Spring Edge AI Midterm Report</h3><footer class=article-time><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--published>Apr 20, 2021</time></footer></div></header><section class=article-content><ul><li><strong>論文名稱：</strong> A CNN-Based Human Head Detection Algorithm Implemented on Edge AI Chip</li><li><strong>論文來源：</strong> ICSSE 2020</li><li><strong>論文連結：</strong> <a href=https://doi.org/10.1109/ICSSE50014.2020.9219260>https://doi.org/10.1109/ICSSE50014.2020.9219260</a></li><li><strong>論文關鍵字：</strong> <code>CNN</code>、<code>人頭檢測</code>、<code>Edge AI晶片</code></li></ul><hr><h2 id=簡介>簡介</h2><p>近年來進行人頭檢測的許多方法都是使用 CNN (Convolutional Neural Network) 進行，但大多數的方法為了追求高準確率，通常都會增加網路的layer的層數或是增加網路的權重，導致計算量增大以及硬體需求增加。</p><p>相對的，使用整合好的 CNN IC 晶片可以得到更好功耗及計算速度，本文使用由 <strong>視芯公司（AVSdep）</strong> 的Mipy (Micropython) 開發板（AVS05P-S），搭配 CNN IC (AI860) 及視芯提供的訓練工具進行模型的訓練。</p><p><figure style=flex-grow:152;flex-basis:365px><a href=/p/edge-cnn-human-head/WQPWzMu.png data-size=646x424><img src=/p/edge-cnn-human-head/WQPWzMu.png srcset="/p/edge-cnn-human-head/WQPWzMu_hud40173a5139939dc4aa8da344b78e686_178835_480x0_resize_box_2.png 480w, /p/edge-cnn-human-head/WQPWzMu_hud40173a5139939dc4aa8da344b78e686_178835_1024x0_resize_box_2.png 1024w" width=646 height=424 loading=lazy alt="Mipy 開發板外觀（圖片來源： https://ieeexplore.ieee.org/document/9219260 ）"></a><figcaption>Mipy 開發板外觀（圖片來源： <a href=https://ieeexplore.ieee.org/document/9219260>https://ieeexplore.ieee.org/document/9219260</a> ）</figcaption></figure><figure style=flex-grow:128;flex-basis:309px><a href=/p/edge-cnn-human-head/lEGzyDu.png data-size=626x486><img src=/p/edge-cnn-human-head/lEGzyDu.png srcset="/p/edge-cnn-human-head/lEGzyDu_hu56c3e12213b55ef3bc2c798a53a11db7_50976_480x0_resize_box_2.png 480w, /p/edge-cnn-human-head/lEGzyDu_hu56c3e12213b55ef3bc2c798a53a11db7_50976_1024x0_resize_box_2.png 1024w" width=626 height=486 loading=lazy alt="Mipy 開發板應用架構圖（圖片來源： https://ieeexplore.ieee.org/document/9219260 ）"></a><figcaption>Mipy 開發板應用架構圖（圖片來源： <a href=https://ieeexplore.ieee.org/document/9219260>https://ieeexplore.ieee.org/document/9219260</a> ）</figcaption></figure></p><h2 id=前置相關工作>前置相關工作</h2><h3 id=訓練工具>訓練工具</h3><p>作者使用由公司提供的訓練工具（C++ 版本）<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>：</p><ol><li><strong>建立資料庫（Create database tool）</strong>
將訓練圖片編碼成二進制檔案以加速訓練時間</li><li><strong>推論工具（Inference tool）</strong>
捕獲各種來源的影像並進行資料擴充（擴充細節請看下節）</li><li><strong>訓練工具（Training tool）</strong>
執行訓練循環（前向傳播、後向傳播、損失計算、權重優化）</li></ol><h3 id=影像資料擴充>影像資料擴充</h3><p>為了要增加模型的準確度以及避免模型過擬合（Overfitting），資料集需要大量且多樣化，因此進行資料擴增：</p><ul><li><strong>旋轉圖片 (Rotation)</strong></li><li><strong>亮度調整 (Brightness adjustment)</strong>
藉由調整圖片像素來進行亮度調整：<ul><li>(1) $P_{new} = P_{old} \times a$</li><li>(2) $P_{new} = P_{old} + b$</li><li>(1) (2) 可以同時進行</li></ul></li><li><strong>模糊處理 (Blurring image)</strong>
隨機模糊或是銳利化圖片</li><li><strong>鏡像處理 (Mirroring image)</strong></li><li><strong>替換背景 (Background replacement)</strong>
將原始圖片中固定的背景顏色替換成隨機的風景圖</li></ul><h2 id=mipy-運算過程>Mipy 運算過程</h2><p><figure style=flex-grow:69;flex-basis:165px><a href=/p/edge-cnn-human-head/7Ndsr2Q.png data-size=536x776><img src=/p/edge-cnn-human-head/7Ndsr2Q.png srcset="/p/edge-cnn-human-head/7Ndsr2Q_hufae0aaffe141195c836de154257d6b63_218947_480x0_resize_box_2.png 480w, /p/edge-cnn-human-head/7Ndsr2Q_hufae0aaffe141195c836de154257d6b63_218947_1024x0_resize_box_2.png 1024w" width=536 height=776 loading=lazy alt="Mipy 運算過程流程圖（圖片來源： https://ieeexplore.ieee.org/document/9219260 ）"></a><figcaption>Mipy 運算過程流程圖（圖片來源： <a href=https://ieeexplore.ieee.org/document/9219260>https://ieeexplore.ieee.org/document/9219260</a> ）</figcaption></figure></p><p>作者拍攝了一些人作為資料集，總共 8 個身體方向，而每個身體方向都包含 5 個頭部方向，分別為：向上、向後、向左、向右、向前，總共拍攝了 800 多張照片，將這些照片分成三個類別，分別為：前面、後面、側面。<figure style=flex-grow:305;flex-basis:733px><a href=/p/edge-cnn-human-head/h3dfONX.png data-size=764x250><img src=/p/edge-cnn-human-head/h3dfONX.png srcset="/p/edge-cnn-human-head/h3dfONX_hucb67491b7a051ba435867d899825433e_268210_480x0_resize_box_2.png 480w, /p/edge-cnn-human-head/h3dfONX_hucb67491b7a051ba435867d899825433e_268210_1024x0_resize_box_2.png 1024w" width=764 height=250 loading=lazy alt="(a) 前面 Front View (b) 後面 Back View (c) 側面 Side View （圖片來源： https://ieeexplore.ieee.org/document/9219260 ）"></a><figcaption>(a) 前面 Front View (b) 後面 Back View (c) 側面 Side View （圖片來源： <a href=https://ieeexplore.ieee.org/document/9219260>https://ieeexplore.ieee.org/document/9219260</a> ）</figcaption></figure></p><p>為了增加資料集的多樣性，作者使用推論工具進行資料擴增（如前面章節說明），將圖片數從800多張擴增到總共 120,000 章圖片，分成 108,000 張圖片為訓練集，12,000 張圖片為測試集，擴增參數設定如表 1。</p><p><figure style=flex-grow:279;flex-basis:671px><a href=/p/edge-cnn-human-head/SZvYLWq.png data-size=806x288><img src=/p/edge-cnn-human-head/SZvYLWq.png srcset="/p/edge-cnn-human-head/SZvYLWq_hue3fd285faa45d1a9777f005bd445d4b4_63390_480x0_resize_box_2.png 480w, /p/edge-cnn-human-head/SZvYLWq_hue3fd285faa45d1a9777f005bd445d4b4_63390_1024x0_resize_box_2.png 1024w" width=806 height=288 loading=lazy alt="資料擴增使用方法及參數（圖片來源： https://ieeexplore.ieee.org/document/9219260 ）"></a><figcaption>資料擴增使用方法及參數（圖片來源： <a href=https://ieeexplore.ieee.org/document/9219260>https://ieeexplore.ieee.org/document/9219260</a> ）</figcaption></figure></p><p>而每次建立資料庫時使用 30,000 張照片作為訓練集資料，3,000 張照片作為測試集資料，並設定輸出神經元類型：</p><table><thead><tr><th>編號</th><th>輸出表示</th></tr></thead><tbody><tr><td>0</td><td>正樣本偵測</td></tr><tr><td>1-4</td><td>類別樣本（其中兩個用來標示前、後、側面）<br>(+1, +1): 前面<br>(+1, -1): 後面<br>(-1, -1): 側面</td></tr><tr><td>5</td><td>負樣本偵測</td></tr><tr><td>6-9</td><td>邊界框位置回歸</td></tr></tbody></table><p>模型超參數如下所示：</p><ul><li>Optimizer: Adam</li><li>Batch size: 250</li><li>Learning rate: 0.0001</li><li>Weight decay: 0.0001</li><li>Recreate database every 30 epoch</li></ul><p>訓練流程使用批次檔進行：</p><ol><li>執行建立資料庫工具</li><li>執行訓練工具</li><li>讀取紀錄訓練工具的 log</li></ol><p>上述步驟進行 3 次後，產生 <code>G0</code> 做為預訓練模型，不過從下圖可以發現訓練誤差遠低於測試誤差。</p><p><figure style=flex-grow:129;flex-basis:311px><a href=/p/edge-cnn-human-head/LVoVSsg.png data-size=410x316><img src=/p/edge-cnn-human-head/LVoVSsg.png srcset="/p/edge-cnn-human-head/LVoVSsg_hud841408a07afb484c3628596b1504c4c_72206_480x0_resize_box_2.png 480w, /p/edge-cnn-human-head/LVoVSsg_hud841408a07afb484c3628596b1504c4c_72206_1024x0_resize_box_2.png 1024w" width=410 height=316 loading=lazy alt="G0 Train/Test Loss （圖片來源： https://ieeexplore.ieee.org/document/9219260 ）"></a><figcaption>G0 Train/Test Loss （圖片來源： <a href=https://ieeexplore.ieee.org/document/9219260>https://ieeexplore.ieee.org/document/9219260</a> ）</figcaption></figure></p><p>因此作者把在風景照中的 False Positive Data（被偵測為人頭但是實際上不是人頭的圖像）是為新的負樣本資料，加入訓練資料中增加模型準確度。</p><p>並設定閥值（Threadshold）來找出需要額外再訓練的圖片，公式如下：
$$S = N_0 - N_5$$</p><ul><li>$N_0$: 正樣本信心分數</li><li>$N_5$: 負樣本信心分數</li></ul><p>這邊訓練的過程就是使用預訓練時的批次檔作為訓練循環，直到大多數的負樣本時將停止訓練，第一次訓練完的模型稱為 <code>G1</code>，作者提到說驗證 <code>G1</code> 的過程中發現許多 False Positive Data，經過重複進行修正和訓練後，第二次訓練完成的模型稱為 <code>G2</code>，以此類推，在 <code>G4</code> 檢測到的 False Positive Data 已經很少，但準確度依舊只有 58.9%，判斷為 overfitting，因此作者決定增加其他訓練資料。<figure style=flex-grow:410;flex-basis:986px><a href=/p/edge-cnn-human-head/1H2L2ER.png data-size=600x146><img src=/p/edge-cnn-human-head/1H2L2ER.png srcset="/p/edge-cnn-human-head/1H2L2ER_hu65ab1b83269040a11217499d11e0ae2d_58434_480x0_resize_box_2.png 480w, /p/edge-cnn-human-head/1H2L2ER_hu65ab1b83269040a11217499d11e0ae2d_58434_1024x0_resize_box_2.png 1024w" width=600 height=146 loading=lazy alt="G1, G2, G3, G4 Train/Test Loss （圖片來源： https://ieeexplore.ieee.org/document/9219260 ）"></a><figcaption>G1, G2, G3, G4 Train/Test Loss （圖片來源： <a href=https://ieeexplore.ieee.org/document/9219260>https://ieeexplore.ieee.org/document/9219260</a> ）</figcaption></figure></p><p>這邊增加的資料為 <strong>Label Face in the Wild (LFW)<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></strong>，挑選的標準為照片中只能出現一個人，以及臉部不可以被其他東西覆蓋，經過篩選後共 9,131 張照片被挑選，其中 1/4 作為測試集，剩餘資料作為訓練集使用。</p><p><figure style=flex-grow:257;flex-basis:617px><a href=/p/edge-cnn-human-head/gMbWiLV.png data-size=355x138><img src=/p/edge-cnn-human-head/gMbWiLV.png srcset="/p/edge-cnn-human-head/gMbWiLV_hu2f0d495c0b7cd110a41685932f9ed5bc_106758_480x0_resize_box_2.png 480w, /p/edge-cnn-human-head/gMbWiLV_hu2f0d495c0b7cd110a41685932f9ed5bc_106758_1024x0_resize_box_2.png 1024w" width=355 height=138 loading=lazy alt="Label Face in the Wild (LFW) 資料示意 （圖片來源： http://vis-www.cs.umass.edu/lfw/#information ）"></a><figcaption>Label Face in the Wild (LFW) 資料示意 （圖片來源： <a href=http://vis-www.cs.umass.edu/lfw/#information>http://vis-www.cs.umass.edu/lfw/#information</a> ）</figcaption></figure></p><p>加入 LFW 訓練後，訓練出了 <code>G5</code>、<code>G6</code> 兩個模型，準確率也分別到達 65.1% 和 70.1%。</p><p><figure style=flex-grow:206;flex-basis:494px><a href=/p/edge-cnn-human-head/1MaGDYH.png data-size=299x145><img src=/p/edge-cnn-human-head/1MaGDYH.png srcset="/p/edge-cnn-human-head/1MaGDYH_hu258a758431b9e6fcfcddca42b6199a38_30228_480x0_resize_box_2.png 480w, /p/edge-cnn-human-head/1MaGDYH_hu258a758431b9e6fcfcddca42b6199a38_30228_1024x0_resize_box_2.png 1024w" width=299 height=145 loading=lazy alt="G5, G6 Train/Test Loss （圖片來源： https://ieeexplore.ieee.org/document/9219260 ）"></a><figcaption>G5, G6 Train/Test Loss （圖片來源： <a href=https://ieeexplore.ieee.org/document/9219260>https://ieeexplore.ieee.org/document/9219260</a> ）</figcaption></figure></p><h2 id=實驗結果與分析>實驗結果與分析</h2><p>將訓練好的模型載入到 AI860 後， Mipy 開發板可以執行人頭偵測任務（如下圖所示）。<figure style=flex-grow:200;flex-basis:480px><a href=/p/edge-cnn-human-head/ME6Aklc.png data-size=450x225><img src=/p/edge-cnn-human-head/ME6Aklc.png srcset="/p/edge-cnn-human-head/ME6Aklc_hu21b3b33ca83fd6f8ed88aebad3cd8268_194226_480x0_resize_box_2.png 480w, /p/edge-cnn-human-head/ME6Aklc_hu21b3b33ca83fd6f8ed88aebad3cd8268_194226_1024x0_resize_box_2.png 1024w" width=450 height=225 loading=lazy alt="實時系統應用畫面（圖片來源： https://ieeexplore.ieee.org/document/9219260 ）"></a><figcaption>實時系統應用畫面（圖片來源： <a href=https://ieeexplore.ieee.org/document/9219260>https://ieeexplore.ieee.org/document/9219260</a> ）</figcaption></figure></p><p>作者也分析了加入 LFW 前後的比較差異，如下表 3、4 所示，我們可以發現準確率提升了 6 %，但是上升幅度並不夠明顯，於是將各種類別（前、後、側面）的資料拆開檢視後發現正面的準確度遠高於側面與後面（如下表 G5/G6 Front View），作者研判原因是因為加入的 LFW dataset 都是正面臉部的關係，導致資料不平衡的情況發生。<figure style=flex-grow:390;flex-basis:937px><a href=/p/edge-cnn-human-head/kBI91So.png data-size=660x169><img src=/p/edge-cnn-human-head/kBI91So.png srcset="/p/edge-cnn-human-head/kBI91So_hu47a4cb1d083bd5acc5d0c7dce528d548_45688_480x0_resize_box_2.png 480w, /p/edge-cnn-human-head/kBI91So_hu47a4cb1d083bd5acc5d0c7dce528d548_45688_1024x0_resize_box_2.png 1024w" width=660 height=169 loading=lazy alt="G4, G5 Confusion Matrix （圖片來源： https://ieeexplore.ieee.org/document/9219260 ）"></a><figcaption>G4, G5 Confusion Matrix （圖片來源： <a href=https://ieeexplore.ieee.org/document/9219260>https://ieeexplore.ieee.org/document/9219260</a> ）</figcaption></figure></p><p><figure style=flex-grow:458;flex-basis:1100px><a href=/p/edge-cnn-human-head/gXWfuKP.png data-size=656x143><img src=/p/edge-cnn-human-head/gXWfuKP.png srcset="/p/edge-cnn-human-head/gXWfuKP_huaeeec9757a7db7fa777adae570aa0be2_32424_480x0_resize_box_2.png 480w, /p/edge-cnn-human-head/gXWfuKP_huaeeec9757a7db7fa777adae570aa0be2_32424_1024x0_resize_box_2.png 1024w" width=656 height=143 loading=lazy alt="G5, G6 Front View Only Confusion Matrix （圖片來源： https://ieeexplore.ieee.org/document/9219260 ）"></a><figcaption>G5, G6 Front View Only Confusion Matrix （圖片來源： <a href=https://ieeexplore.ieee.org/document/9219260>https://ieeexplore.ieee.org/document/9219260</a> ）</figcaption></figure></p><h2 id=結論>結論</h2><p>本文利用的視芯公司（AVSdep）提供的訓練工具訓練出模型後，部署在開發板及 CNN IC 晶片上後，可以順利執行人頭辨識的任務（Human Head Detection）。其中訓練完的模型正面臉部的準確率可以達到 98.7%，模型應用在實時的檢測系統上也足夠準確，使用完整的訓練資料時，Mipy 開發板可以獲得足夠的準確率以執行實際應用。</p><h2 id=個人心得>個人心得</h2><p>筆者覺得這篇論文有點像是視芯公司的業配文，主要實作細節並沒有闡述太多關於 CNN IC 晶片網路架構的部分，另外在 <code>G1</code> ~ <code>G4</code> 的訓練過程中，testing loss 也都並沒有下降，感覺 False Positive Correction （誤報修正）並沒有太大作用，加入 LFW 後才有明顯改善，但效果仍然有待加強，後文在分析準確率上升問題時，很可惜的並沒有看到像是 <code>G4 (Front View)</code> 或是 <code>G5 Back/Side View</code> 的 Confusion Matrix 來做比較，導致只能聽從作者分析結論並沒有實驗數據參佐。</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Advance Video System CO., LTD (AVSdsp), AI courses, requirements, tool updates, Q&A area: CNN Tool v0.0.1.2c, Available: <a href=http://www.avsdsp.com/AI_Data.html>http://www.avsdsp.com/AI_Data.html</a> <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller, “Labeled faces in the wild: A database for studying face recognition in unconstrained environments,” Technical Report, University of Massachusetts, Amherst, Oct. 2007. <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></section><footer class=article-footer><section class=article-tags><a href=/tags/cnn/>cnn</a>
<a href=/tags/ntust/>ntust</a>
<a href=/tags/human-head-detection/>human head detection</a>
<a href=/tags/edge-ai-chip/>edge ai chip</a>
<a href=/tags/report/>report</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}]});})</script></article><aside class=related-contents--wrapper><h2 class=section-title>Related contents</h2><div class=related-contents><div class="flex article-list--tile"><article class=has-image><a href=/p/edge-plate-detection/><div class=article-image><img src=/p/edge-plate-detection/BNgrLe5.79bf412e098ed48782d69c8f96f25eab_huf738ae938586e2523874cb2afd528823_128500_250x150_fill_box_smart1_2.png width=250 height=150 loading=lazy data-key=edge-plate-detection data-hash="md5-eb9BLgmO1IeC1pyPlvJeqw=="></div><div class=article-details><h2 class=article-title>利用車牌辨識以檢測違規車牌</h2></div></a></article><article class=has-image><a href=/p/fake-news-detection-safe/><div class=article-image><img src=/p/fake-news-detection-safe/_hu1fe59100f806a3b1884406926f185a64_1483991_8b3315783814951b31c1e93cb4659713.png width=250 height=150 loading=lazy data-key=fake-news-detection-safe data-hash="md5-juJaf/mARgWZCV8XJLt3iQ=="></div><div class=article-details><h2 class=article-title>Similarity-Aware Multi-Modal Fake News Detection</h2></div></a></article><article><a href=/p/ir-homework5/><div class=article-details><h2 class=article-title>Query Modeling</h2></div></a></article><article class=has-image><a href=/p/ir-homework4/><div class=article-image><img src=/p/ir-homework4/qITG12G.5671d03356598a720234f4b848c1d20b_hu1dffc869e1d64a4036e5075742de8ba2_29927_250x150_fill_box_smart1_2.png width=250 height=150 loading=lazy data-key=ir-homework4 data-hash="md5-VnHQM1ZZinICNPS4SMHSCw=="></div><div class=article-details><h2 class=article-title>PLSA</h2></div></a></article><article><a href=/p/ir-homework2/><div class=article-details><h2 class=article-title>Best Match Model 25 (BM25)</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=chiachun2491/blog issue-term=pathname label=comments crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>function setUtterancesTheme(theme){let utterances=document.querySelector('.utterances iframe');if(utterances){utterances.contentWindow.postMessage({type:'set-theme',theme:`github-${theme}`},'https://utteranc.es');}}
addEventListener('message',event=>{if(event.origin!=='https://utteranc.es')return;setUtterancesTheme(document.documentElement.dataset.scheme)});window.addEventListener('onColorSchemeChange',(e)=>{setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2019 -
2021 Jeffery's Blog</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=2.5.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css integrity="sha256-c0uckgykQ9v5k+IqViZOZKc47Jn7KQil4/MP3ySA3F8=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE=" crossorigin=anonymous></main><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#簡介>簡介</a></li><li><a href=#前置相關工作>前置相關工作</a><ol><li><a href=#訓練工具>訓練工具</a></li><li><a href=#影像資料擴充>影像資料擴充</a></li></ol></li><li><a href=#mipy-運算過程>Mipy 運算過程</a></li><li><a href=#實驗結果與分析>實驗結果與分析</a></li><li><a href=#結論>結論</a></li><li><a href=#個人心得>個人心得</a></li></ol></nav></div></section></aside></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin=anonymous defer></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const customFont=document.createElement('link');customFont.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";customFont.type="text/css";customFont.rel="stylesheet";document.head.appendChild(customFont);}());</script></body></html>