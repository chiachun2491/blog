<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>information retrieval on Jeffery's Blog</title><link>https://blog.jefferyho.cc/tags/information-retrieval/</link><description>Recent content in information retrieval on Jeffery's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-tw</language><lastBuildDate>Thu, 17 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.jefferyho.cc/tags/information-retrieval/index.xml" rel="self" type="application/rss+xml"/><item><title>Query Modeling</title><link>https://blog.jefferyho.cc/p/ir-homework5/</link><pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate><guid>https://blog.jefferyho.cc/p/ir-homework5/</guid><description>Kaggle competitions 2020: Information Retrieval and Applications
Homework 5: Query Modeling
https://www.kaggle.com/c/2020-information-retrieval-and-applications-hw5
Github code https://github.com/chiachun2491/NTUST_IR/tree/master/homework5
Homework report 使用的 tool Python, numpy, pandas, collections.Counter, scipy.sparse, numba.jit, datetime
資料前處理 將 doc_list.txt, query_list.txt 讀檔進來後，之後將每個 doc 和 query 使用 collections.Counter 儲存。 生成 document 和 query 的 tf-idf Lexicon 生成方式：使用 df 範圍（5 ~ 10000）的單字，過濾一些 stop word 和稀少的單字。 在 c(w,d), c(w,q) 使用 sublinear_tf，並先計算好 document 和 query 的 unigram language model，也先計算好 background language model。 作業流程 使用 vsm 做為第一次檢索的結果去做 rocchio，再將 rocchio 最好的結果作為 smm relevant document，下去做 smm</description></item><item><title>PLSA</title><link>https://blog.jefferyho.cc/p/ir-homework4/</link><pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate><guid>https://blog.jefferyho.cc/p/ir-homework4/</guid><description>Kaggle competitions 2020: Information Retrieval and Applications
Homework 4: PLSA
https://www.kaggle.com/c/2020-information-retrieval-and-applications-hw4-v2
Github code https://github.com/chiachun2491/NTUST_IR/tree/master/homework4
Homework report 使用的 tool Python, Jupyter, numpy, pandas, collections.Counter, scipy.sparse, numba.jit, datetime
資料前處理 將 doc_list.txt, query_list.txt 讀檔進來後，之後將每個 doc 使用 collections.Counter 儲存到 dict，每個 query 都使用 split() 儲存到 dict。 這次 Lexicon 的生成方式跟之前不一樣，之前在生成 Lexicon 時只看 query_list 的所有單詞，這次先將 doc 和 query 出現過的詞加入 Lexicon 並先計算好 document length, c(w, d), P(w|d), P(w|BG) 先算好供後面算 term weight 使用。為了加速運算，決定還是減少單字的數量，只取出現次數遞減取前 10000 個單字，再把 query 的字也加進去。 PLSA 模型參數調整 PLSA term weight 公式：</description></item><item><title>Best Match Model 25 (BM25)</title><link>https://blog.jefferyho.cc/p/ir-homework2/</link><pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate><guid>https://blog.jefferyho.cc/p/ir-homework2/</guid><description>Kaggle competitions 2020: Information Retrieval and Applications
Homework2: Best Match Models
https://www.kaggle.com/c/2020-information-retrieval-and-applications-hw2
Github code https://github.com/chiachun2491/NTUST_IR/tree/master/homework2
Homework report 使用的 tool Python, Jupyter, numpy, dataframe, datetime
資料前處理 將 doc_list.txt 和 query_list.txt 讀檔進來後，之後將每個 doc 和 query 都使用 split() 儲存起來。 跟上次 Vector Space Model 的作業一樣，在生成 Lexicon 時只看 query_list 切完的所有詞並放到 set 中來生成，這樣能把 Lexicon 的維度從 59680 降到僅 123 而已。 跟上次作業相同先將 document term-frequency, query term-frequency, inverse document frequency 先算好供後面算 BM25 term weight 使用。 BM25 模型參數調整 BM25 term weight 公式： $$sim_{BM25}\left (d_{j}, q \right ) \equiv \sum_{w_{j}\in \left ( d_{j} \cap q \right )}^{} IDF(w_{j}) \times \frac{ \left ( K_{1} + 1 \right ) \times tf_{i,j}}{K_{1} [\left ( 1 - b \right ) + b \times \frac{len\left ( d_{j} \right )}{avg_{doclen}}] + tf_{i,j}} \times \frac{\left ( K_{3} + 1 \right ) \times tf_{i,q}}{ K_{3} + tf_{i,q}}$$</description></item><item><title>Vector Space Model</title><link>https://blog.jefferyho.cc/p/ir-homework1/</link><pubDate>Sun, 25 Oct 2020 00:00:00 +0000</pubDate><guid>https://blog.jefferyho.cc/p/ir-homework1/</guid><description>Kaggle competitions 2020: Information Retrieval and Applications
Homework1: Vector Space Model
https://www.kaggle.com/c/2020-information-retrieval-and-applications/
Github code https://github.com/chiachun2491/NTUST_IR/tree/master/homework1
Homework report 使用的 tool Python, Jupyter, numpy, dataframe, sklearn.metrics.pairwise.cosine_similarity, datetime
資料前處理 一開始我將 doc_list 和 query_list 讀檔進來後，之後將每個 doc 和 query 都使用 split() 儲存起來，並將這些 list 放到 set 中製作 dictionary，這邊做了一個小偷吃步，直接把 dictionary 的範圍縮小到只看所有 query 出現過的字，把 dictionary 的維度從 59680 降到了 123 而已。
模型參數調整 我的 document 和 query 的 term weight 都是使用此公式：
$$tf_{i,j} \times log(1+\frac{N+1}{n_{i}+1})$$
模型運作原理 vector space model 會給出一個 doc 或 query 對應到所有 dictionary 中的向量，所以我們使用一個 doc 的向量和 query 的向量去做 cosine similarity，我們就可以得到兩者間的相似程度，所以我們將所有 doc 的向量都跟 query 的向量算過相似度後，我們就可以從高排到低找出跟該 query 最相關的 doc。</description></item></channel></rss>