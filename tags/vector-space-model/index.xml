<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>vector space model on Jeffery's Blog</title><link>https://blog.jefferyho.cc/tags/vector-space-model/</link><description>Recent content in vector space model on Jeffery's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-tw</language><lastBuildDate>Sun, 25 Oct 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.jefferyho.cc/tags/vector-space-model/index.xml" rel="self" type="application/rss+xml"/><item><title>Vector Space Model</title><link>https://blog.jefferyho.cc/p/ir-homework1/</link><pubDate>Sun, 25 Oct 2020 00:00:00 +0000</pubDate><guid>https://blog.jefferyho.cc/p/ir-homework1/</guid><description>Kaggle competitions 2020: Information Retrieval and Applications
Homework1: Vector Space Model
https://www.kaggle.com/c/2020-information-retrieval-and-applications/
Github code https://github.com/chiachun2491/NTUST_IR/tree/master/homework1
Homework report 使用的 tool Python, Jupyter, numpy, dataframe, sklearn.metrics.pairwise.cosine_similarity, datetime
資料前處理 一開始我將 doc_list 和 query_list 讀檔進來後，之後將每個 doc 和 query 都使用 split() 儲存起來，並將這些 list 放到 set 中製作 dictionary，這邊做了一個小偷吃步，直接把 dictionary 的範圍縮小到只看所有 query 出現過的字，把 dictionary 的維度從 59680 降到了 123 而已。
模型參數調整 我的 document 和 query 的 term weight 都是使用此公式：
$$tf_{i,j} \times log(1+\frac{N+1}{n_{i}+1})$$
模型運作原理 vector space model 會給出一個 doc 或 query 對應到所有 dictionary 中的向量，所以我們使用一個 doc 的向量和 query 的向量去做 cosine similarity，我們就可以得到兩者間的相似程度，所以我們將所有 doc 的向量都跟 query 的向量算過相似度後，我們就可以從高排到低找出跟該 query 最相關的 doc。</description></item></channel></rss>